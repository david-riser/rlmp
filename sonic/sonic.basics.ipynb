{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Technical Things To Do\n",
    "- Setup nuts/bolts and try imitation learning on one level (done)\n",
    "- Optimize imitation learning training on one level\n",
    "- Setup nuts/bolts and try imitation learning on all levels, test generalization (not strictly necessary for the goal of this work)\n",
    "- Specify a generic evaluation protocol that can be used for all agents\n",
    "    - Fixed frame score over a number of evaluation attempts at the same level, mean.\n",
    "- Try learning from scratch with an RL, record the mean episodic reward throughout training.\n",
    "- Use the same agent but pre-trained with human demonstrations, record the mean episodic reward throughout training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import gym\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import retro\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "from gym import wrappers\n",
    "from IPython.display import clear_output\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "levels = [\n",
    "    'MarbleZone.Act1','MarbleZone.Act2','MarbleZone.Act3',\n",
    "    'GreenHillZone.Act1','GreenHillZone.Act2','GreenHillZone.Act3',\n",
    "    'LabyrinthZone.Act1','LabyrinthZone.Act2','LabyrinthZone.Act3', \n",
    "    'SpringYardZone.Act1','SpringYardZone.Act2','SpringYardZone.Act3', \n",
    "    'StarLightZone.Act1','StarLightZone.Act2','StarLightZone.Act3',\n",
    "    'ScrapBrainZone.Act1','ScrapBrainZone.Act2' \n",
    "]\n",
    "\n",
    "config = {}\n",
    "config['data_path'] = os.path.abspath(\"./data/human\")\n",
    "config['level'] = 'GreenHillZone.Act1'\n",
    "\n",
    "config['epochs'] = 20\n",
    "config['batches_per_epoch'] = 40\n",
    "config['batch_size'] = 32\n",
    "config['patience'] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_human_data(movie_path):\n",
    "    \"\"\" Given a path to a replay file, load it and \n",
    "        extract the series of state-action pairs.\n",
    "    \"\"\"\n",
    "    movie = retro.Movie(movie_path)\n",
    "    movie.step()\n",
    "\n",
    "    env = retro.make(game=movie.get_game(), state=retro.State.NONE, \n",
    "                     use_restricted_actions=retro.Actions.ALL)\n",
    "    env.initial_state = movie.get_state()\n",
    "    state = env.reset()\n",
    "\n",
    "    states, actions, next_states, rewards, dones = [], [], [], [], []\n",
    "    while movie.step():\n",
    "        keys = []\n",
    "        for i in range(len(env.buttons)):\n",
    "            keys.append(movie.get_key(i, 0))\n",
    "            \n",
    "        next_state, reward, done, info = env.step(keys)\n",
    "        actions.append(np.int8(keys))\n",
    "        states.append(state)\n",
    "        next_states.append(next_state)\n",
    "        rewards.append(reward)\n",
    "        dones.append(done)\n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "    return states, actions, next_states, rewards, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, actions, next_states, rewards, dones = extract_human_data(f\"data/human/SonicTheHedgehog-Genesis-{config['level']}-0000.bk2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 1 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 1 0 0 0 0]\n",
      " [0 1 0 0 1 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0]] [1841  881  224  139   38   21   18   16    5    4    3    1]\n"
     ]
    }
   ],
   "source": [
    "unique_actions, counts = np.unique(actions, axis=0, return_counts=True)\n",
    "\n",
    "ordering = np.argsort(counts)[::-1]\n",
    "unique_actions = unique_actions[ordering]\n",
    "counts = counts[ordering]\n",
    "print(unique_actions, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identified 12 unique actions of 4096 possible.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Identified {len(counts)} unique actions of {2**12} possible.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduced Action Space\n",
    "Humans are only displaying a very limited subset of the total number of actions possible.  Lets take advantage of this by encoding the action space into a simple integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_actions(actions):\n",
    "    \"\"\" Cast actions to a string and then use a label encoder to \n",
    "        transform the vector into a simple integer.  \n",
    "    \"\"\"\n",
    "    str_actions = [\"\".join(str(action)) for action in actions]\n",
    "    encoder = LabelEncoder()\n",
    "    return encoder.fit_transform(str_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "actions = encode_actions(actions)\n",
    "n_actions = len(np.unique(actions))\n",
    "print(n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.layers.Input(states[0].shape)\n",
    "last_layer = tf.keras.layers.Conv2D(filters=16, kernel_size=3, strides=1, activation='relu')(inputs)\n",
    "last_layer = tf.keras.layers.Conv2D(filters=16, kernel_size=3, strides=3, activation='relu')(last_layer)\n",
    "last_layer = tf.keras.layers.Conv2D(filters=16, kernel_size=3, strides=1, activation='relu')(last_layer)\n",
    "last_layer = tf.keras.layers.Conv2D(filters=16, kernel_size=3, strides=3, activation='relu')(last_layer)\n",
    "last_layer = tf.keras.layers.Conv2D(filters=16, kernel_size=3, strides=1, activation='relu')(last_layer)\n",
    "last_layer = tf.keras.layers.Conv2D(filters=16, kernel_size=3, strides=3, activation='relu')(last_layer)\n",
    "flat_layer = tf.keras.layers.Flatten()(last_layer)\n",
    "last_layer = tf.keras.layers.Dense(256, activation='relu')(flat_layer)\n",
    "outputs = tf.keras.layers.Dense(n_actions, activation='softmax')(last_layer)\n",
    "model = tf.keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 320, 3)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 222, 318, 16)      448       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 74, 106, 16)       2320      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 72, 104, 16)       2320      \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 24, 34, 16)        2320      \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 22, 32, 16)        2320      \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 7, 10, 16)         2320      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1120)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               286976    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 12)                3084      \n",
      "=================================================================\n",
      "Total params: 302,108\n",
      "Trainable params: 302,108\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-3), \n",
    "    loss='sparse_categorical_crossentropy'\n",
    ")\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    \n",
    "    def __init__(self, max_epochs=10, batch_size=32, \n",
    "                 batches_per_epoch=100, patience=4):\n",
    "        self.max_epochs = max_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.batches_per_epoch = batches_per_epoch\n",
    "        self.train_loss = []\n",
    "        self.test_loss = []\n",
    "        self.best_loss = np.inf\n",
    "        self.best_weights = None\n",
    "        self.patience = patience\n",
    "        \n",
    "    def train(self, model, x_train, y_train, x_test, y_test):  \n",
    "        \n",
    "        down_rounds = 0\n",
    "        for epoch in range(self.max_epochs):\n",
    "            epoch_loss = 0\n",
    "            for batch in range(self.batches_per_epoch):\n",
    "                indices = np.random.choice(np.arange(x_train.shape[0]), self.batch_size)\n",
    "                loss = model.train_on_batch(x_train[indices], y_train[indices])\n",
    "                epoch_loss += loss / self.batches_per_epoch\n",
    "        \n",
    "            epoch_test_loss = model.evaluate(x_test, y_test)\n",
    "            self.train_loss.append(epoch_loss)\n",
    "            self.test_loss.append(epoch_test_loss)\n",
    "            print(\"Epoch loss: \", epoch_loss)\n",
    "            print(\"Dev. loss: \", epoch_test_loss)\n",
    "            \n",
    "            if epoch_test_loss > self.best_loss:\n",
    "                down_rounds += 1\n",
    "            else:\n",
    "                self.best_loss = epoch_test_loss\n",
    "                self.best_weights = model.get_weights()\n",
    "                down_rounds = 0\n",
    "                \n",
    "            if down_rounds >= self.patience:\n",
    "                return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "798/798 [==============================] - 1s 791us/sample - loss: 1.0605\n",
      "Epoch loss:  1.3870005920529367\n",
      "Dev. loss:  1.0605337022241195\n",
      "798/798 [==============================] - 0s 528us/sample - loss: 0.9586\n",
      "Epoch loss:  0.7874593630433081\n",
      "Dev. loss:  0.9586434014757773\n",
      "798/798 [==============================] - 0s 523us/sample - loss: 0.8633\n",
      "Epoch loss:  0.6253519490361212\n",
      "Dev. loss:  0.8632723577040479\n",
      "798/798 [==============================] - 0s 527us/sample - loss: 0.7640\n",
      "Epoch loss:  0.41440807580947875\n",
      "Dev. loss:  0.7640444830545506\n",
      "798/798 [==============================] - 0s 527us/sample - loss: 0.7290\n",
      "Epoch loss:  0.39366494938731195\n",
      "Dev. loss:  0.728963992051911\n",
      "798/798 [==============================] - 0s 534us/sample - loss: 0.6632\n",
      "Epoch loss:  0.30408192612230783\n",
      "Dev. loss:  0.6632122887405836\n",
      "798/798 [==============================] - 0s 532us/sample - loss: 0.5436\n",
      "Epoch loss:  0.19394124317914246\n",
      "Dev. loss:  0.5435722948315748\n",
      "798/798 [==============================] - 0s 521us/sample - loss: 0.5415\n",
      "Epoch loss:  0.17872789893299346\n",
      "Dev. loss:  0.5415477149916771\n",
      "798/798 [==============================] - 0s 528us/sample - loss: 0.7035\n",
      "Epoch loss:  0.11961138658225538\n",
      "Dev. loss:  0.7034769126944673\n",
      "798/798 [==============================] - 0s 527us/sample - loss: 0.6104\n",
      "Epoch loss:  0.12294472735375164\n",
      "Dev. loss:  0.6103961613393367\n",
      "798/798 [==============================] - 0s 523us/sample - loss: 0.5853\n",
      "Epoch loss:  0.1053288206923753\n",
      "Dev. loss:  0.5852927195099661\n",
      "798/798 [==============================] - 0s 524us/sample - loss: 0.5888\n",
      "Epoch loss:  0.16890363353304566\n",
      "Dev. loss:  0.5887751532975295\n",
      "798/798 [==============================] - 0s 527us/sample - loss: 0.6192\n",
      "Epoch loss:  0.1045202625566162\n",
      "Dev. loss:  0.6192346224211213\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(np.array(states), np.array(actions))\n",
    "\n",
    "trainer = ModelTrainer(\n",
    "    max_epochs=config['epochs'],\n",
    "    batch_size=config['batch_size'],\n",
    "    batches_per_epoch=config['batches_per_epoch'],\n",
    "    patience=config['patience']\n",
    ")\n",
    "trainer.train(model, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_weights(trainer.best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(trainer.train_loss, color='blue', label='Train')\n",
    "plt.plot(trainer.test_loss, color='red', label='Test')\n",
    "plt.grid(alpha=0.2)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.savefig(\"imitation_losses.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = retro.make(game='SonicTheHedgehog-Genesis', \n",
    "                 state=config['level'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_imitation_game(env, model, rnd_steps=50, max_frames=10000):\n",
    "    \"\"\" Docstring.\n",
    "    \"\"\"\n",
    "    \n",
    "    state = env.reset()\n",
    "    frames = []\n",
    "    frames.append(env.render(mode='rgb_array'))\n",
    "    for step in range(rnd_steps):\n",
    "        state, _, _, _ = env.step(env.action_space.sample())\n",
    "        frames.append(env.render(mode='rgb_array'))\n",
    "        \n",
    "    done = False\n",
    "    while not done:\n",
    "        \n",
    "        # Act according to our policy most of the time.  It \n",
    "        # seems to get stuck so I am trying to add some random\n",
    "        # elements to get Sonic unstuck.\n",
    "        if step % 5 == 0:\n",
    "            new_state, reward, done, info = env.step(env.action_space.sample())\n",
    "            state = new_state\n",
    "        \n",
    "        else:\n",
    "            action = model.predict(state.reshape(1, *state.shape))\n",
    "            action = np.round(action[0])\n",
    "            action = np.array(action, dtype=np.int8)\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "            state = new_state\n",
    "            \n",
    "        frames.append(env.render(mode='rgb_array'))\n",
    "        if len(frames) % 500 == 0:\n",
    "            print(\"Collected {} frames\".format(len(frames)))\n",
    "        \n",
    "        if len(frames) >= max_frames:\n",
    "            return frames\n",
    "        \n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 500 frames\n",
      "Collected 1000 frames\n",
      "Collected 1500 frames\n",
      "Collected 2000 frames\n",
      "Collected 2500 frames\n",
      "Collected 3000 frames\n",
      "Collected 3500 frames\n",
      "Collected 4000 frames\n",
      "Collected 4500 frames\n",
      "Collected 5000 frames\n",
      "Collected 5500 frames\n",
      "Collected 6000 frames\n",
      "Collected 6500 frames\n",
      "Collected 7000 frames\n",
      "Collected 7500 frames\n"
     ]
    }
   ],
   "source": [
    "frames = record_imitation_game(env, model, rnd_steps=50, max_frames=7500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import imageio\n",
    "except:\n",
    "    !pip install imageio\n",
    "    !pip install imageio-ffmpeg\n",
    "    import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageio.mimwrite(\n",
    "    \"imitation_agent_{}.mp4\".format(config[\"level\"]), \n",
    "    frames, fps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable([1., 2., 3.])\n",
    "y = tf.Variable([2., 0., 3.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float32, numpy=array([3., 2., 6.], dtype=float32)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.math.add(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float32, numpy=array([1., 4., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.math.squared_difference(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=1.6666666>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.math.reduce_mean(tf.math.squared_difference(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_mat = tf.Variable([[1., 2., 3.], [3., 6., -1.]])\n",
    "y_mat = tf.Variable([[4., -2., 3.], [0., 2., 4.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[5., 0., 6.],\n",
       "       [3., 8., 3.]], dtype=float32)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.math.add(x_mat, y_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = tf.random.uniform(shape=(4, 10))\n",
    "next_states = tf.random.uniform(shape=(4, 10))\n",
    "actions = tf.constant(np.random.randint(0, 4, size=(4,1)))\n",
    "rewards = tf.random.uniform(shape=(4,1))\n",
    "dones = tf.constant([False, False, True, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_inputs = tf.keras.layers.Input(10)\n",
    "fake_outputs = tf.keras.layers.Dense(4)(fake_inputs)\n",
    "fake_model = tf.keras.Model(fake_inputs, fake_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.4171334 ,  0.6635723 , -0.88306516, -0.22167009],\n",
       "       [-0.8014855 ,  0.19262055, -0.52876467, -0.6003244 ],\n",
       "       [-1.0505676 ,  0.46929562, -1.3963273 , -0.92107165],\n",
       "       [-1.1203967 ,  0.309789  , -1.3208236 , -1.0710857 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_model.predict(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_pred_states = fake_model.predict(states)\n",
    "q_next_states = fake_model.predict(next_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 1), dtype=float32, numpy=\n",
       "array([[0.42383397],\n",
       "       [0.23711133],\n",
       "       [0.80573773],\n",
       "       [0.31680262]], dtype=float32)>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_terminals = tf.reshape((1. - tf.cast(dones, dtype=tf.float32)), (4,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 1), dtype=float32, numpy=\n",
       "array([[0.42383397],\n",
       "       [0.23711133],\n",
       "       [0.        ],\n",
       "       [0.31680262]], dtype=float32)>"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards * not_terminals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.0539377 ,  0.37638736, -0.9314213 , -1.2616074 ],\n",
       "       [-0.08313143,  1.4039065 , -0.91746664, -0.77965534],\n",
       "       [-0.6801462 ,  0.3467161 , -1.3542186 , -0.7677697 ],\n",
       "       [-0.6651447 ,  0.66179264, -1.271966  , -1.2616999 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_next_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=int64, numpy=array([1, 1, 1, 1])>"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.math.argmax(q_next_states, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = tf.math.argmax(q_next_states, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.37638736, 0.37638736, 0.37638736, 0.37638736],\n",
       "       [1.4039065 , 1.4039065 , 1.4039065 , 1.4039065 ],\n",
       "       [0.3467161 , 0.3467161 , 0.3467161 , 0.3467161 ],\n",
       "       [0.66179264, 0.66179264, 0.66179264, 0.66179264]], dtype=float32)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_next_states[:,indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=float32, numpy=array([0.37638736, 1.4039065 , 0.3467161 , 0.66179264], dtype=float32)>"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.math.reduce_max(q_next_states, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 4), dtype=float32, numpy=\n",
       "array([[0.7964574 , 1.8137014 , 0.7670829 , 1.0790087 ],\n",
       "       [0.6097348 , 1.6269788 , 0.5803603 , 0.89228606],\n",
       "       [0.37262347, 1.3898674 , 0.34324893, 0.65517473],\n",
       "       [0.68942606, 1.70667   , 0.6600516 , 0.97197735]], dtype=float32)>"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards * not_terminals + 0.99 * tf.math.reduce_max(q_next_states, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 4), dtype=float32, numpy=\n",
       "array([[-0.4171334 ,  0.6635723 , -0.88306516, -0.22167009],\n",
       "       [-0.4171334 ,  0.6635723 , -0.88306516, -0.22167009],\n",
       "       [-0.4171334 ,  0.6635723 , -0.88306516, -0.22167009],\n",
       "       [-0.4171334 ,  0.6635723 , -0.88306516, -0.22167009]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.gather(q_pred_states, tf.math.argmax(actions, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 1), dtype=int64, numpy=\n",
       "array([[3],\n",
       "       [3],\n",
       "       [2],\n",
       "       [3]])>"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 0 0 0], shape=(4,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "indices = tf.math.argmax(actions, axis=1)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.4171334 ,  0.6635723 , -0.88306516, -0.22167009],\n",
       "       [-0.8014855 ,  0.19262055, -0.52876467, -0.6003244 ],\n",
       "       [-1.0505676 ,  0.46929562, -1.3963273 , -0.92107165],\n",
       "       [-1.1203967 ,  0.309789  , -1.3208236 , -1.0710857 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_pred_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 4), dtype=float32, numpy=\n",
       "array([[-0.4171334, -0.4171334, -0.4171334, -0.4171334],\n",
       "       [-0.8014855, -0.8014855, -0.8014855, -0.8014855],\n",
       "       [-1.0505676, -1.0505676, -1.0505676, -1.0505676],\n",
       "       [-1.1203967, -1.1203967, -1.1203967, -1.1203967]], dtype=float32)>"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.gather(q_pred_states, indices, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 4), dtype=float32, numpy=\n",
       "array([[-0.4171334, -0.4171334, -0.4171334, -0.4171334],\n",
       "       [-0.8014855, -0.8014855, -0.8014855, -0.8014855],\n",
       "       [-1.0505676, -1.0505676, -1.0505676, -1.0505676],\n",
       "       [-1.1203967, -1.1203967, -1.1203967, -1.1203967]], dtype=float32)>"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.gather(q_pred_states, indices, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 1, 4), dtype=float32, numpy=\n",
       "array([[[-0.4171334 ,  0.6635723 , -0.88306516, -0.22167009]],\n",
       "\n",
       "       [[-0.4171334 ,  0.6635723 , -0.88306516, -0.22167009]],\n",
       "\n",
       "       [[-0.4171334 ,  0.6635723 , -0.88306516, -0.22167009]],\n",
       "\n",
       "       [[-0.4171334 ,  0.6635723 , -0.88306516, -0.22167009]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.gather(q_pred_states, tf.expand_dims(indices, axis=1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.0539377 ,  0.37638736, -0.9314213 , -1.2616074 ],\n",
       "       [-0.08313143,  1.4039065 , -0.91746664, -0.77965534],\n",
       "       [-0.6801462 ,  0.3467161 , -1.3542186 , -0.7677697 ],\n",
       "       [-0.6651447 ,  0.66179264, -1.271966  , -1.2616999 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_next_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1 1 1 1], shape=(4,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "indices = tf.math.argmax(q_next_states, axis=1)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 4), dtype=float32, numpy=\n",
       "array([[-0.08313143,  1.4039065 , -0.91746664, -0.77965534],\n",
       "       [-0.08313143,  1.4039065 , -0.91746664, -0.77965534],\n",
       "       [-0.08313143,  1.4039065 , -0.91746664, -0.77965534],\n",
       "       [-0.08313143,  1.4039065 , -0.91746664, -0.77965534]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.gather(q_next_states, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 4)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_next_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 4), dtype=float32, numpy=\n",
       "array([[-0.08313143,  1.4039065 , -0.91746664, -0.77965534],\n",
       "       [-0.08313143,  1.4039065 , -0.91746664, -0.77965534],\n",
       "       [-0.08313143,  1.4039065 , -0.91746664, -0.77965534],\n",
       "       [-0.08313143,  1.4039065 , -0.91746664, -0.77965534]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.gather_nd(q_next_states, tf.expand_dims(indices, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=int64, numpy=array([1, 1, 1, 1])>"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 4), dtype=float32, numpy=\n",
       "array([[-0.08313143,  1.4039065 , -0.91746664, -0.77965534],\n",
       "       [-0.6801462 ,  0.3467161 , -1.3542186 , -0.7677697 ],\n",
       "       [-0.6801462 ,  0.3467161 , -1.3542186 , -0.7677697 ],\n",
       "       [-0.08313143,  1.4039065 , -0.91746664, -0.77965534]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.gather_nd(q_next_states, [[1],[2], [2], [1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.37638736, 0.37638736, 0.37638736, 0.37638736],\n",
       "       [1.4039065 , 1.4039065 , 1.4039065 , 1.4039065 ],\n",
       "       [0.3467161 , 0.3467161 , 0.3467161 , 0.3467161 ],\n",
       "       [0.66179264, 0.66179264, 0.66179264, 0.66179264]], dtype=float32)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_next_states[:,indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare row indices\n",
    "indices = tf.cast(indices, tf.int32)\n",
    "row_indices = tf.range(tf.shape(indices)[0])\n",
    "\n",
    "# zip row indices with column indices\n",
    "full_indices = tf.stack([row_indices, indices], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 2), dtype=int32, numpy=\n",
       "array([[0, 1],\n",
       "       [1, 2],\n",
       "       [2, 2],\n",
       "       [3, 1]], dtype=int32)>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.1398852 ,  0.6970059 ,  0.6675799 , -0.0247772 ],\n",
       "       [-1.7801754 ,  0.47129062,  1.1793231 , -0.7775614 ],\n",
       "       [-1.5888344 , -0.04008031,  0.9824762 , -0.17618082],\n",
       "       [-1.150228  ,  0.19246128,  0.17176041, -0.68164104]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_next_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=float32, numpy=array([0.6970059 , 1.1793231 , 0.9824762 , 0.19246128], dtype=float32)>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.gather_nd(q_next_states, full_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def td_loss(online_model, target_model, states, actions, next_states, rewards, dones, gamma):\n",
    "    \"\"\" Calculate the temporal difference loss. \"\"\"\n",
    "    \n",
    "    print(\"Actions: \", actions)\n",
    "    \n",
    "    # Predict the value of the current and next states using the appropriate \n",
    "    # networks for each. \n",
    "    q_pred_states = online_model.predict(states)\n",
    "    q_next_states = target_model.predict(next_states)\n",
    "    \n",
    "    # If the current state is the terminal state, the reward for the transition\n",
    "    # is provided by there is no future reward.  The state value is zero regardless \n",
    "    # which action would be chosen.  \n",
    "    not_terminals = tf.reshape((1. - tf.cast(dones, dtype=tf.float32)), (-1,1))\n",
    "    \n",
    "    # Select the target indices for update based on the actions \n",
    "    # taken by the agent.\n",
    "    batch_size = tf.shape(dones)[0]\n",
    "    row_indices = tf.range(batch_size)\n",
    "    print(\"Row indices: \", row_indices)\n",
    "    update_indices = tf.stack([row_indices, tf.squeeze(tf.cast(actions, tf.int32))], axis=1)\n",
    "    \n",
    "    # Select maximum actions from the next state for the TD upate.\n",
    "    indices = tf.stack([row_indices, tf.cast(tf.argmax(q_next_states), tf.int32)])\n",
    "    q_max_next_states = q_next_states[indices]\n",
    "    \n",
    "    # Finally, we can build the target Q-value for the current state\n",
    "    # based on the TD-error.\n",
    "    print(\"Update indices: \", update_indices)\n",
    "    print(\"Q-next: \", q_next_states)\n",
    "    q_target_states = q_next_states\n",
    "    q_target_states[update_indices] = rewards + not_terminals * q_max_next_states * gamma\n",
    "    print(\"Q target: \", q_target_states)\n",
    "    \n",
    "    # Calculate the temporal difference loss \n",
    "    # loss = tf.reduce_mean(tf.squared_difference(q_pred_states, q_target_states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions:  tf.Tensor(\n",
      "[[3]\n",
      " [3]\n",
      " [2]\n",
      " [3]], shape=(4, 1), dtype=int64)\n",
      "Row indices:  tf.Tensor([0 1 2 3], shape=(4,), dtype=int32)\n",
      "Update indices:  tf.Tensor(\n",
      "[[0 3]\n",
      " [1 3]\n",
      " [2 2]\n",
      " [3 3]], shape=(4, 2), dtype=int32)\n",
      "Q-next:  [[-1.0539377   0.37638736 -0.9314213  -1.2616074 ]\n",
      " [-0.08313143  1.4039065  -0.91746664 -0.77965534]\n",
      " [-0.6801462   0.3467161  -1.3542186  -0.7677697 ]\n",
      " [-0.6651447   0.66179264 -1.271966   -1.2616999 ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/ipykernel/__main__.py:25: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "/home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/ipykernel/__main__.py:32: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-172-b92a60e533a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtd_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-171-57c324eafb61>\u001b[0m in \u001b[0;36mtd_loss\u001b[0;34m(online_model, target_model, states, actions, next_states, rewards, dones, gamma)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Q-next: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_next_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mq_target_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_next_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mq_target_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mupdate_indices\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnot_terminals\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mq_max_next_states\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Q target: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_target_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "td_loss(fake_model, fake_model, states, actions, next_states, rewards, dones, gamma=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.0539377 ,  0.37638736, -0.9314213 , -1.2616074 ],\n",
       "       [-0.08313143,  1.4039065 , -0.91746664, -0.77965534],\n",
       "       [-0.6801462 ,  0.3467161 , -1.3542186 , -0.7677697 ],\n",
       "       [-0.6651447 ,  0.66179264, -1.271966  , -1.2616999 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_next_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.4171334 ,  0.6635723 , -0.88306516, -0.22167009],\n",
       "       [-0.8014855 ,  0.19262055, -0.52876467, -0.6003244 ],\n",
       "       [-1.0505676 ,  0.46929562, -1.3963273 , -0.92107165],\n",
       "       [-1.1203967 ,  0.309789  , -1.3208236 , -1.0710857 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_pred_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 1), dtype=int64, numpy=\n",
       "array([[3],\n",
       "       [3],\n",
       "       [2],\n",
       "       [3]])>"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'update_indices' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-176-b067283ab33a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mupdate_indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'update_indices' is not defined"
     ]
    }
   ],
   "source": [
    "def keras_td_loss(online_model, target_model, states, actions, next_states, rewards, dones, gamma):\n",
    "    \"\"\" Docstring. \"\"\"\n",
    "    \n",
    "    # Predict state-action values for the current state and \n",
    "    # the next state using the online and target networks. \n",
    "    q_pred_states = online_model.predict(states)\n",
    "    q_next_states = target_model.predict(next_states)\n",
    "    \n",
    "    # Compute the TD-error\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.4171334 ,  0.6635723 , -0.88306516, -0.22167009],\n",
       "       [-0.8014855 ,  0.19262055, -0.52876467, -0.6003244 ],\n",
       "       [-1.0505676 ,  0.46929562, -1.3963273 , -0.92107165],\n",
       "       [-1.1203967 ,  0.309789  , -1.3208236 , -1.0710857 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_pred_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.0539377 ,  0.37638736, -0.9314213 , -1.2616074 ],\n",
       "       [-0.08313143,  1.4039065 , -0.91746664, -0.77965534],\n",
       "       [-0.6801462 ,  0.3467161 , -1.3542186 , -0.7677697 ],\n",
       "       [-0.6651447 ,  0.66179264, -1.271966  , -1.2616999 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_next_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3],\n",
       "       [3],\n",
       "       [2],\n",
       "       [3]])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_indices = np.arange(q_next_states.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3] [[3]\n",
      " [3]\n",
      " [2]\n",
      " [3]]\n"
     ]
    }
   ],
   "source": [
    "print(row_indices, actions.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-1.2616074 ],\n",
       "        [-1.2616074 ],\n",
       "        [-0.9314213 ],\n",
       "        [-1.2616074 ]],\n",
       "\n",
       "       [[-0.77965534],\n",
       "        [-0.77965534],\n",
       "        [-0.91746664],\n",
       "        [-0.77965534]],\n",
       "\n",
       "       [[-0.7677697 ],\n",
       "        [-0.7677697 ],\n",
       "        [-1.3542186 ],\n",
       "        [-0.7677697 ]],\n",
       "\n",
       "       [[-1.2616999 ],\n",
       "        [-1.2616999 ],\n",
       "        [-1.271966  ],\n",
       "        [-1.2616999 ]]], dtype=float32)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_next_states[:, actions.numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow2_p36)",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
